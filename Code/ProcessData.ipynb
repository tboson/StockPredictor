{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for this project is taken from the ML competition by G-Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data manipulation\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# machine learning\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "#xgboost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from data files\n",
    "df = pd.read_csv('../Data/train.csv')\n",
    "dt = pd.read_csv('../Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of NaN values\n",
    "#Find out which columns has null values\n",
    "columns = df.columns\n",
    "for i in range(len(df.columns)):\n",
    "    index = df.index[df[columns[i]].isnull()]\n",
    "    if index.shape[0]>0:\n",
    "        print(df.columns[i],index.shape)\n",
    "        df[columns[i]] = df[columns[i]].fillna(0)\n",
    "columns2 = dt.columns\n",
    "for i in range(len(dt.columns)):\n",
    "    index2 = dt.index[dt[columns2[i]].isnull()]\n",
    "    if index2.shape[0]>0:\n",
    "        print(dt.columns[i],index2.shape)\n",
    "        dt[columns2[i]] = dt[columns2[i]].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training dataset and test dataset\n",
    "x_train = df.drop(['Weight','y'],axis = 1)\n",
    "x_weight = np.array(df.Weight)\n",
    "y_train = df.y\n",
    "x_test = dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the best stopping point\n",
    "def BestStoppingRounds(XGBmodel,\n",
    "                       x_train,\n",
    "                       y_train,\n",
    "                       weight,\n",
    "                       eval_metric = 'mae'):\n",
    "    result_set = []\n",
    "    kf = KFold(n_splits=10)\n",
    "    fold = 0\n",
    "    for train_index, test_index in kf.split(x_train):\n",
    "        fold += 1\n",
    "        xfold_train, xfold_test = x_train.values[train_index], x_train.values[test_index]\n",
    "        yfold_train, yfold_test = y_train.values[train_index], y_train.values[test_index]\n",
    "        xfold_weight = weight[train_index]\n",
    "        eval_set = ([xfold_train,yfold_train],[xfold_test, yfold_test])\n",
    "        XGBmodel.fit(xfold_train,\n",
    "                     yfold_train,\n",
    "                     sample_weight = xfold_weight,\n",
    "                     early_stopping_rounds = 10,\n",
    "                     eval_metric=[eval_metric],\n",
    "                     eval_set=eval_set,\n",
    "                     verbose=True)\n",
    "        # evaluate predictions\n",
    "        best_score = XGBmodel.best_score\n",
    "        best_iteration = XGBmodel.best_iteration\n",
    "        result_set.append([best_score, XGBmodel.best_iteration])\n",
    "        print(\"Minimum \" + eval_metric + \" : %f\" %best_score)\n",
    "        print(\"Best iteration: %d\" %best_iteration)\n",
    "    return result_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune XGBoost hyperparameters\n",
    "# Part 1: set a low learning rate and find the best stopping point\n",
    "\n",
    "XGBmodel = XGBRegressor(learning_rate = 0.1,\n",
    "                        n_estimators = 1000)\n",
    "\n",
    "result_set = BestStoppingRounds(XGBmodel, x_train, y_train, x_weight, 'mae')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the best stopping point\n",
    "result_array = np.array(result_set)\n",
    "plt.plot(result_array[:,1],result_array[:,0])\n",
    "#take the mean of stopping points\n",
    "mean_mae = result_array[:,0].mean()\n",
    "best_stopping = int(result_array[:,1].mean())\n",
    "print(\"best_stopping: %d\" %best_stopping)\n",
    "print(\"mean mae: %f\" %mean_mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tune parameters\n",
    "def XGBTuneParam(XGBmodel,\n",
    "                 param_grid,\n",
    "                 x_train,\n",
    "                 y_train,\n",
    "                 weight):\n",
    "\n",
    "    xgbcv = GridSearchCV(XGBmodel,param_grid)\n",
    "    if type(weight)==str:\n",
    "        xgbcv.fit(x_train,y_train)\n",
    "    else:\n",
    "        xgbcv.fit(x_train,y_train,weight)\n",
    "    return xgbcv, xgbcv.best_score_, xgbcv.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Tune max_depth and mean_child_weight\n",
    "# best fit parameter: learning_rate = 0.1, max_depth = 6, min_child_weight = 2\n",
    "XGBmodel = XGBRegressor(learning_rate = 0.1, \n",
    "                        n_estimators = best_stopping\n",
    "                       )\n",
    "param_grid = { \n",
    "        \"max_depth\"             : [2, 4, 6],\n",
    "        \"min_child_weight\"      : [2, 4, 8]\n",
    "        }\n",
    "\n",
    "Tune2model, Tune2score, Tune2params = XGBTuneParam(XGBmodel,\n",
    "                                                   param_grid,\n",
    "                                                   x_train,\n",
    "                                                   y_train,\n",
    "                                                   x_weight)\n",
    "\n",
    "Tuned_max_depth = Tune2params['max_depth']\n",
    "Tuned_min_child_weight = Tune2params['min_child_weight']\n",
    "print(\"max_depth %d\" %Tune2params['max_depth'])\n",
    "print(\"min_child_weight %f\" %Tune2params['min_child_weight'])\n",
    "print(\"Accuracy_score %f\" %Tune2score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Tune gamma\n",
    "# best fit parameter: gamma = 0.0\n",
    "XGBmodel = Tune2model\n",
    "\n",
    "param_grid = { \n",
    "        \"gamma\"             : [0.0, 0.1, 0.2]\n",
    "        }\n",
    "\n",
    "Tune3model, Tune3score, Tune3params = XGBTuneParam(XGBmodel,\n",
    "                                                   param_grid,\n",
    "                                                   x_train,\n",
    "                                                   y_train,\n",
    "                                                   x_weight) \n",
    "Tuned_gamma = Tune3params[\"gamma\"]\n",
    "print(\"gamma %f\" %Tuned_gamma)\n",
    "print(\"Accuracy_score %f\" %Tune3score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisit best stopping rounds\n",
    "\n",
    "XGBmodel = XGBRegressor(learning_rate = 0.1,\n",
    "                        gamma = Tuned_gamma,\n",
    "                        max_depth = Tuned_max_depth,\n",
    "                        min_child_weight = Tuned_min_child_weight,\n",
    "                        n_estimators = 1000)\n",
    "     \n",
    "result_set = BestStoppingRounds(XGBmodel, x_train, y_train, x_weight)\n",
    "\n",
    "#Find the best stopping point\n",
    "result_array = np.array(result_set)\n",
    "plt.plot(result_array[:,1],result_array[:,0])\n",
    "#take the mean of stopping points\n",
    "mean_mae = result_array[:,0].mean()\n",
    "best_stopping = int(result_array[:,1].mean())\n",
    "print(\"best_stopping: %d\" %best_stopping)\n",
    "print(\"mean mae: %f\" %mean_mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4: Tune colsample_bytree and subsample\n",
    "# best fit parameter: colsample_bytree = 0.7, subsample = 1\n",
    "XGBmodel = XGBRegressor(learning_rate = 0.1,\n",
    "                       gamma = Tuned_gamma,\n",
    "                       max_depth = Tuned_max_depth,\n",
    "                       min_child_weight = Tuned_min_child_weight,\n",
    "                       n_estimators= best_stopping)\n",
    "\n",
    "param_grid = { \n",
    "        \"subsample\"         : [i/10 for i in range(7,11)],\n",
    "        \"colsample_bytree\"  : [i/10 for i in range(7,11)]\n",
    "        }\n",
    "\n",
    "Tune4model, Tune4score, Tune4params = XGBTuneParam(XGBmodel,\n",
    "                                                   param_grid,\n",
    "                                                   x_train,\n",
    "                                                   y_train,\n",
    "                                                   x_weight)\n",
    "Tuned_subsample = Tune4params['subsample']\n",
    "Tuned_colsample_bytree = Tune4params['colsample_bytree']\n",
    "print(\"subsample %f\" %Tuned_subsample)\n",
    "print(\"colsample_bytree %f\" %Tuned_colsample_bytree)\n",
    "print(\"Accuracy_score %f\" %Tune4score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 5: Tune learning rate\n",
    "# The slower the learning rate, the more boosting rounds will be needed, and the longer\n",
    "# it will take to train\n",
    "# Find the optimal trade off between time and error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def XGBTunePart4(XGBmodel, x_train, y_train, weight):\n",
    "    param_grid = { \n",
    "            \"subsample\"             : [0.6,0.8,1],\n",
    "            \"colsample_bytree\"      : [0.6,0.8,1],\n",
    "            }\n",
    "    xgbcv = GridSearchCV(XGBmodel,param_grid)\n",
    "    if type(weight)==str:\n",
    "        xgbcv.fit(x_train,y_train)\n",
    "    else:\n",
    "        xgbcv.fit(x_train,y_train,weight)\n",
    "    return xgbcv.best_score_, xgbcv.best_params_\n",
    "Tune4model, Tune4score, Tune4params = XGBTunePart4(XGBmodel, x_train,y_train,x_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 6: XGBoost with all tuned parameters\n",
    "    XGBmodel = XGBRegressor(learning_rate = 0.1, \n",
    "                            n_estimators = best_stopping,\n",
    "                            max_depth = 6,\n",
    "                            min_child_weight = 4,\n",
    "                            colsample_bytree = 0.8,\n",
    "                            subsmaple = 1\n",
    "                           )\n",
    "XGBmodel.fit(x_train, y_train, x_weight)\n",
    "print(XGBmodel.score(x_train, y_train, x_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict result with trained model\n",
    "yp = pd.Series(XGBmodel.predict(x_test)).rename('y')\n",
    "yp.index.name = 'Index'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predicted data to csv\n",
    "yp.to_csv('XGBoostRegressor_model.csv', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
